# MCP (Model Context Protocol) Server Configuration
# Sample configuration for Contrastive Goat MCP server

# Server Configuration
server:
  name: "contrastive-goat-mcp"
  version: "1.0.0"
  description: "MCP server for Contrastive Goat learning and inference"
  host: "0.0.0.0"
  port: 8000
  
  # Enable development mode for detailed logging and hot reload
  debug: true
  
  # CORS settings for web client integration
  cors:
    enabled: true
    origins:
      - "http://localhost:5173"  # Vite dev server
      - "http://localhost:4173"  # Vite preview server
      - "https://thebearwithabite.github.io"  # Production deployment
    methods: ["GET", "POST", "PUT", "DELETE", "OPTIONS"]
    headers: ["Content-Type", "Authorization", "X-Request-ID"]

# Model Configuration
models:
  # Contrastive learning model for goat image pairs
  contrastive_goat:
    type: "contrastive"
    architecture: "resnet50"
    input_size: [224, 224, 3]
    embedding_size: 512
    temperature: 0.07
    
    # Model files and weights
    weights_path: "./models/contrastive_goat.pth"
    config_path: "./models/contrastive_goat_config.json"
    
    # Inference settings
    batch_size: 32
    device: "auto"  # auto-detect GPU/CPU
    precision: "fp16"  # fp32, fp16, or mixed
    
    # Preprocessing
    preprocessing:
      normalize: true
      mean: [0.485, 0.456, 0.406]
      std: [0.229, 0.224, 0.225]
      resize: 256
      crop_size: 224

  # Feelings classification model
  feelings_classifier:
    type: "classification"
    architecture: "transformer"
    num_classes: 8
    input_modality: "text_image"
    
    weights_path: "./models/feelings_classifier.pth"
    vocab_path: "./models/feelings_vocab.json"
    
    batch_size: 16
    max_sequence_length: 512
    device: "auto"

  # Future frame prediction model
  frame_predictor:
    type: "generative"
    architecture: "diffusion"
    input_frames: 4
    output_frames: 1
    resolution: [256, 256]
    
    weights_path: "./models/frame_predictor.pth"
    scheduler_config: "./models/scheduler_config.json"
    
    batch_size: 8
    guidance_scale: 7.5
    num_inference_steps: 50
    device: "auto"

# Data Configuration
data:
  # Dataset paths
  datasets:
    goat_pairs: "./data/goat_pairs.json"
    feelings_items: "./data/feelings_items.json"
    predict_sets: "./data/predict_sets.json"
  
  # Data loading settings
  num_workers: 4
  prefetch_factor: 2
  pin_memory: true
  
  # Data validation
  validate_on_load: true
  require_alt_text: true
  max_image_size_mb: 5

# Logging Configuration
logging:
  level: "INFO"  # DEBUG, INFO, WARNING, ERROR, CRITICAL
  
  # Log outputs
  console:
    enabled: true
    format: "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
    
  file:
    enabled: true
    path: "./logs/mcp_server.log"
    max_size_mb: 100
    backup_count: 5
    format: "%(asctime)s - %(name)s - %(levelname)s - %(funcName)s:%(lineno)d - %(message)s"
  
  # Structured logging for analysis
  json:
    enabled: false
    path: "./logs/mcp_server.jsonl"

# Performance Monitoring
monitoring:
  # Metrics collection
  metrics:
    enabled: true
    port: 8001
    path: "/metrics"
    
  # Request tracking
  tracing:
    enabled: true
    sample_rate: 0.1
    jaeger_endpoint: null  # Set if using Jaeger
    
  # Health checks
  health:
    enabled: true
    path: "/health"
    include_model_status: true

# Security Configuration
security:
  # API key authentication (if needed)
  api_key:
    enabled: false
    header_name: "X-API-Key"
    required_for_paths: ["/v1/models", "/v1/inference"]
  
  # Rate limiting
  rate_limiting:
    enabled: true
    requests_per_minute: 60
    burst_size: 10
  
  # Input validation
  validation:
    max_request_size_mb: 10
    max_batch_size: 64
    allowed_image_types: ["jpg", "jpeg", "png", "webp"]

# Cache Configuration
cache:
  # Model cache
  models:
    enabled: true
    max_size_gb: 4
    ttl_hours: 24
    
  # Inference cache
  inference:
    enabled: true
    max_size_gb: 1
    ttl_minutes: 30
    
  # Data cache
  data:
    enabled: true
    max_size_gb: 2
    ttl_hours: 6

# Development Settings
development:
  # Auto-reload on code changes
  auto_reload: true
  
  # Mock mode for testing without models
  mock_mode: false
  
  # Profiling
  profiling:
    enabled: false
    output_dir: "./profiles"
    
  # Testing endpoints
  test_endpoints:
    enabled: true
    path: "/test"

# Integration Settings
integrations:
  # Database (if needed for user data, analytics)
  database:
    enabled: false
    url: "postgresql://localhost:5432/contrastive_goat"
    pool_size: 10
    
  # Message queue (if needed for async processing)
  message_queue:
    enabled: false
    broker_url: "redis://localhost:6379/0"
    
  # External APIs
  external_apis:
    huggingface:
      enabled: false
      api_key: null
      base_url: "https://api-inference.huggingface.co"